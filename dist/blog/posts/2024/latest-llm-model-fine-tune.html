<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>主流llm大模型微调小结 | YiMing's blog</title>
    <meta name="description" content="llm大模型微调总结">
    <link rel="preload stylesheet" href="/assets/style.cb8d09bc.css" as="style">
    <script type="module" src="/assets/app.bb2119bd.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.2ed14f66.woff2" as="font" type="font/woff2" crossorigin="">
  <link rel="modulepreload" href="/assets/chunks/framework.41330901.js">
  <link rel="modulepreload" href="/assets/chunks/theme.500c9978.js">
  <link rel="modulepreload" href="/assets/blog_posts_2024_latest-llm-model-fine-tune.md.f2e579b3.lean.js">
  <link rel="icon" href="yiminglogo2.png">
  <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-2bf54a99><!--[--><!--]--><!--[--><span tabindex="-1" data-v-f5a352f2></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-f5a352f2> Skip to content </a><!--]--><!----><header class="VPNav" data-v-2bf54a99 data-v-283c37a7><div class="VPNavBar" data-v-283c37a7 data-v-2264840b><div class="container" data-v-2264840b><div class="title" data-v-2264840b><div class="VPNavBarTitle" data-v-2264840b data-v-d298091b><a class="title" href="/" data-v-d298091b><!--[--><!--]--><!--[--><img class="VPImage logo" src="/yiminglogo2.png" alt data-v-6dd5884f><!--]--><!--[-->YiMing&#39;s Blog<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-2264840b><div class="curtain" data-v-2264840b></div><div class="content-body" data-v-2264840b><!--[--><!--]--><div class="VPNavBarSearch search" style="--vp-meta-key:&#39;Meta&#39;;" data-v-2264840b><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20" aria-label="search icon"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-2264840b data-v-f54c34e6><span id="main-nav-aria-label" class="visually-hidden" data-v-f54c34e6>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-f54c34e6 data-v-bdc88584 data-v-d502cd7b><!--[-->Home<!--]--><!----></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-f54c34e6 data-v-529736ff><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-529736ff><span class="text" data-v-529736ff><!----> Blog <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-529736ff><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-529736ff><div class="VPMenu" data-v-529736ff data-v-280e28cb><div class="items" data-v-280e28cb><!--[--><!--[--><div class="VPMenuLink" data-v-280e28cb data-v-2a9f32f4><a class="VPLink link" href="/blog/" data-v-2a9f32f4 data-v-d502cd7b><!--[-->Blog Home<!--]--><!----></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-280e28cb data-v-2a9f32f4><a class="VPLink link" href="/blog/tags.html" data-v-2a9f32f4 data-v-d502cd7b><!--[-->Tags<!--]--><!----></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-280e28cb data-v-2a9f32f4><a class="VPLink link" href="/blog/archives.html" data-v-2a9f32f4 data-v-d502cd7b><!--[-->Archives<!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/authors/yi-ming.html" tabindex="0" data-v-f54c34e6 data-v-bdc88584 data-v-d502cd7b><!--[-->About<!--]--><!----></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-2264840b data-v-0088fcc3><label title="toggle dark mode" data-v-0088fcc3 data-v-6efeb7e0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-checked="false" data-v-6efeb7e0 data-v-5c77962f><span class="check" data-v-5c77962f><span class="icon" data-v-5c77962f><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-6efeb7e0><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-6efeb7e0><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></label></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-2264840b data-v-f8334a3a data-v-c74e4df9><!--[--><a class="VPSocialLink" href="https://github.com/realzhengyiming" aria-label="github" target="_blank" rel="noopener" data-v-c74e4df9 data-v-d6790091><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-2264840b data-v-206ebc7c data-v-529736ff><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-529736ff><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-529736ff><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-529736ff><div class="VPMenu" data-v-529736ff data-v-280e28cb><!----><!--[--><!--[--><!----><div class="group" data-v-206ebc7c><div class="item appearance" data-v-206ebc7c><p class="label" data-v-206ebc7c>Appearance</p><div class="appearance-action" data-v-206ebc7c><label title="toggle dark mode" data-v-206ebc7c data-v-6efeb7e0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-checked="false" data-v-6efeb7e0 data-v-5c77962f><span class="check" data-v-5c77962f><span class="icon" data-v-5c77962f><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-6efeb7e0><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-6efeb7e0><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></label></div></div></div><div class="group" data-v-206ebc7c><div class="item social-links" data-v-206ebc7c><div class="VPSocialLinks social-links-list" data-v-206ebc7c data-v-c74e4df9><!--[--><a class="VPSocialLink" href="https://github.com/realzhengyiming" aria-label="github" target="_blank" rel="noopener" data-v-c74e4df9 data-v-d6790091><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-2264840b data-v-47edab87><span class="container" data-v-47edab87><span class="top" data-v-47edab87></span><span class="middle" data-v-47edab87></span><span class="bottom" data-v-47edab87></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav" data-v-2bf54a99 data-v-b59ad933><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-b59ad933 data-v-c20a59a7><button data-v-c20a59a7>Return to top</button><!----></div></div><!----><div class="VPContent" id="VPContent" data-v-2bf54a99 data-v-e1441724><div class="VPDoc has-aside" data-v-e1441724 data-v-e031c361><!--[--><!--]--><div class="container" data-v-e031c361><div class="left-aside aside" data-v-e031c361><div class="aside-curtain" data-v-e031c361></div><div class="aside-container" data-v-e031c361><div class="aside-content" data-v-e031c361><div class="VPDocAside" data-v-e031c361 data-v-6db944aa><!--[--><!--[--><!--[--><!--[--><!--[--><span class="bg-primary-100 inline-flex items-center rounded text-sm font-medium"><div data-v-262d93ff><div class="i-[heroicons-outline/annotation] mr-2" data-v-262d93ff></div><span data-v-262d93ff>Document</span></div></span><span class="bg-primary-100 inline-flex rounded text-sm font-medium"><div class="flex flex-wrap gap-2 py-5"><!--[--><a class="rounded-sm bg-gray-100 px-2 py-1 text-xs font-semibold text-gray-600" href="/blog/tags?init=深度学习"><!----> 深度学习</a><!--]--></div></span><dl class="pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 dark:xl:border-slate-200/5" data-v-f51709bc><dt class="sr-only" data-v-f51709bc>Authors</dt><dd data-v-f51709bc><ul class="flex justify-center space-x-8 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8" data-v-f51709bc><li class="flex items-center space-x-2" data-v-f51709bc><img src="https://avatars.githubusercontent.com/u/26131338?v=4" alt="author image" class="h-10 w-10 rounded-full" data-v-f51709bc><dl class="whitespace-nowrap text-sm font-medium leading-5" data-v-f51709bc><dt class="sr-only" data-v-f51709bc>Name</dt><dd class="text-gray-900 dark:text-white" data-v-f51709bc><a href="/blog/authors/yi-ming.html" class="text-lg text-gray-900 hover:text-[color:var(--vp-c-brand-light)] dark:text-white dark:hover:text-[color:var(--vp-c-brand-dark)]" data-v-f51709bc>Yi Ming</a></dd><!----><!----></dl></li></ul></dd></dl><!--]--><!--]--><!--]--><!--]--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-6db944aa data-v-12ef9dee><div class="content" data-v-12ef9dee><div class="outline-marker" data-v-12ef9dee></div><div class="outline-title" data-v-12ef9dee>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-12ef9dee><span class="visually-hidden" id="doc-outline-aria-label" data-v-12ef9dee> Table of Contents for current page </span><ul class="root" data-v-12ef9dee data-v-df1a6cf2><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-6db944aa></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--[--><!--[--><!--[--><footer class="mb-24 divide-y divide-gray-200 text-sm font-medium leading-5 dark:divide-slate-200/5" data-v-2f3a5683><!----><div class="py-3" data-v-2f3a5683><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-white" data-v-2f3a5683> Previous Article </h2><div class="link" data-v-2f3a5683><a href="/blog/posts/2024/Ollama_Local_Inference&amp;Summary_Generation_Capability.html" data-v-2f3a5683>Ollama本地推理&amp;文本标题生成</a></div></div><div class="pt-3" data-v-2f3a5683><a class="link" href="/blog/" data-v-2f3a5683>← Back to the blog</a></div></footer><!----><!--]--><!--]--><!--]--><!--]--></div></div></div></div><div class="content" data-v-e031c361><div class="content-container" data-v-e031c361><!--[--><!--[--><!--[--><!--[--><header class="space-y-1 pt-6 text-center xl:pb-10"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-300"><time datetime="2024-02-25T12:00:00.000Z">February 25, 2024</time></dd></dl><h1 class="md:leading-14 text-3xl font-extrabold leading-9 tracking-tight text-[color:var(--vp-c-brand-dark)] dark:text-[color:var(--vp-c-brand-light)] sm:text-4xl sm:leading-10 md:text-5xl">主流llm大模型微调小结</h1></header><!--[--><div class="xs:show xl:hidden flex justify-center space-x-8 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8"><span class="bg-primary-100 inline-flex items-center rounded text-sm font-medium"><div data-v-262d93ff><div class="i-[heroicons-outline/annotation] mr-2" data-v-262d93ff></div><span data-v-262d93ff>Document</span></div></span></div><dl class="pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 dark:xl:border-slate-200/5 xs:show xl:hidden" data-v-f51709bc><dt class="sr-only" data-v-f51709bc>Authors</dt><dd data-v-f51709bc><ul class="flex justify-center space-x-8 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8" data-v-f51709bc><li class="flex items-center space-x-2" data-v-f51709bc><img src="https://avatars.githubusercontent.com/u/26131338?v=4" alt="author image" class="h-10 w-10 rounded-full" data-v-f51709bc><dl class="whitespace-nowrap text-sm font-medium leading-5" data-v-f51709bc><dt class="sr-only" data-v-f51709bc>Name</dt><dd class="text-gray-900 dark:text-white" data-v-f51709bc><a href="/blog/authors/yi-ming.html" class="text-lg text-gray-900 hover:text-[color:var(--vp-c-brand-light)] dark:text-white dark:hover:text-[color:var(--vp-c-brand-dark)]" data-v-f51709bc>Yi Ming</a></dd><!----><!----></dl></li></ul></dd></dl><!--]--><!--]--><!----><!--]--><!--]--><!--]--><!----><main class="main" data-v-e031c361><div style="position:relative;" class="vp-doc _blog_posts_2024_latest-llm-model-fine-tune" data-v-e031c361><div><h2 id="引言" tabindex="-1">引言 <a class="header-anchor" href="#引言" aria-label="Permalink to &quot;引言&quot;">​</a></h2><p>llm大模型目前比较火的两个垂直领域应用的技术路线: PEFT(参数有效微调)和RAG(检索增强生成). 目前这两个方向按实用性来说, 如果单看垂直领域知识库, 那确实RAG能以更低的成本和更快的速度应用起来.<br> 但是<strong>微调训练</strong>能为大语言模型增加新的功能, 比如翻译, 使用agent(agent工具调用微调), 所以它依然非常重要, 并且微调技术的增强和RAG是可以共同促进的. 下文就以简单的身份微调作为例子, 简单实践下llm微调.</p><hr><h2 id="大模型微调技术路线" tabindex="-1">大模型微调技术路线 <a class="header-anchor" href="#大模型微调技术路线" aria-label="Permalink to &quot;大模型微调技术路线&quot;">​</a></h2><h3 id="sft-有监督微调" tabindex="-1">SFT (有监督微调) <a class="header-anchor" href="#sft-有监督微调" aria-label="Permalink to &quot;SFT (有监督微调)&quot;">​</a></h3><p>简单的说就是<strong>传统全参数微调</strong>, 特点就是, 标记数据中将输入数据映射到期望的输出去. 计算成本低于预训练(从训练数据的量来说, 但是对于大模型来说,这个量依然很大) 例子:</p><ul><li>ImageNet数据集预训练后, 通过较少的数据集迁移到其他主题的数据集上的图像识别模型</li><li>stable diffusion 的dreambooth微调(全参)</li><li>llm fine-tune (比如bert 一般就是基于这种方式全参微调)</li></ul><h3 id="peft-参数有效微调" tabindex="-1">PEFT (参数有效微调) <a class="header-anchor" href="#peft-参数有效微调" aria-label="Permalink to &quot;PEFT (参数有效微调)&quot;">​</a></h3><p>旨在减少在微调过程中需要更新的参数数量, 通常是在微调的时候固定模型的一部分参数, 只更新一部分参数, 或者在原模型中增加一个可调的小型网络来对模型进行微调. 例子:</p><ul><li>Prefix Tuning</li></ul><ul><li>ptuning</li><li>ptuning v2</li><li>LoRA</li><li>stable diffusion 中的 text-inversion 、 hyperNetwork 、lora等技术</li><li>...</li></ul><h3 id="rlhf-人类反馈的强化学习" tabindex="-1">RLHF (人类反馈的强化学习) <a class="header-anchor" href="#rlhf-人类反馈的强化学习" aria-label="Permalink to &quot;RLHF (人类反馈的强化学习)&quot;">​</a></h3><p>chatgpt中用到的一种基于强化学习方法的技术. 激励模型训练，使语言模型补全与人工标签员的偏好保持一致。类似于, 水平高的人类围棋手就能够训练出水平高的围棋选手一样. rlhf也是, 通过人类对生成结果的偏好选择进行反馈, 然后让llm能够生成更多人类偏好的内容.  例子:</p><ul><li>chatgpt 重要的微调流程</li></ul><h2 id="目前比较主流的peft微调" tabindex="-1">目前比较主流的PEFT微调 <a class="header-anchor" href="#目前比较主流的peft微调" aria-label="Permalink to &quot;目前比较主流的PEFT微调&quot;">​</a></h2><p>目前的PEFT微调方法也还在不停的更新中, 不过截止到此文编写只是, lora应该是比较主流的. 对于llm大模型, 还有ptuning v2</p><h3 id="lora微调" tabindex="-1">LoRA微调 <a class="header-anchor" href="#lora微调" aria-label="Permalink to &quot;LoRA微调&quot;">​</a></h3><p>LoRA通过添加一个低秩线性适应层到原有模型，冻结原模型权重, 只对新增加的低秩矩阵进行微调，大大降低了微调的复杂度。lora微调的结构图如下, 它有效的原因是模型往往是过参数化的, 大部分的任务只需要改动少量权重(和对应新任务相关的权重) 就应该可以有效果. LoRA 不会改变层的所有组件中的权重矩阵 W，而是创建两个较小的矩阵 A 和 B，其乘积大致表示对 W 的修改。适应可以在数学上表示为 Y = W+AB. 通过微调AB就可以让模型适配新的任务.</p><p><img src="/assets/lora微调注入结构.8a777aea.png" alt=""></p><h3 id="qlora" tabindex="-1">QLoRA <a class="header-anchor" href="#qlora" aria-label="Permalink to &quot;QLoRA&quot;">​</a></h3><p>QLoRA是 LoRA 的扩展版本，它的工作原理是将预训练LLM的权重参数的精度量化为 4 位精度。通常，训练模型的参数以 32 位格式存储，但 QLoRA 将它们压缩为 4 位格式。这减少了 LLM的内存占用，从而可以在单个 GPU 上对其进行微调。这种方法显著减少了内存占用，使得在功能较弱的硬件（包括消费类 GPU）上运行LLM模型成为可能。 <img src="/assets/对比lora-qlora.50c36916.png" alt=""></p><h3 id="lora-与qlora的对比" tabindex="-1">LoRA 与QLoRA的对比 <a class="header-anchor" href="#lora-与qlora的对比" aria-label="Permalink to &quot;LoRA 与QLoRA的对比&quot;">​</a></h3><p>LoRA优点:</p><ul><li>允许高效的模型微调，只需要更新模型中的小部分参数。</li><li>提供了参数扩展的灵活性，而不需要大规模改动原有的权重。<br> LoRA缺点:</li><li>相对于量化版本，存储和计算资源需求更高。</li><li>对于资源有限的环境来说，可能不如量化方法高效。<br> QLoRA优点:</li><li>通过量化减少了模型的内存占用和计算资源使用。</li><li>提高了模型在资源受限设备上的适用性和能效。<br> QLoRA缺点:</li><li>量化可能会引起模型精度的损失，尤其是在低比特宽度量化时。</li><li>需要仔细调优量化级别，以及可能在某些硬件上不提供性能优势。</li><li>并且QLoRA由于采用了低精度, 训练难度也会大一些</li></ul><h3 id="ptuning-v2" tabindex="-1">ptuning v2 <a class="header-anchor" href="#ptuning-v2" aria-label="Permalink to &quot;ptuning v2&quot;">​</a></h3><p>主流还是LoRA系列的. ptunning 目前好像主要chatglm在用. ptunning v2 相比于ptuning v1增加了注入的层数, 让ptuning在参数量10b以下的模型也能达到还不错的效果.</p><h2 id="llm-lora-训练" tabindex="-1">llm LoRA 训练 <a class="header-anchor" href="#llm-lora-训练" aria-label="Permalink to &quot;llm LoRA 训练&quot;">​</a></h2><ul><li>可以参考不同的模型的各自训练微调的说明</li><li>huggingface PEFT库</li><li>带Webui的 <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noreferrer">llama-factory</a> 体验也还可以</li><li>etc...</li></ul><h3 id="llama-factory-llama-board-微调模型" tabindex="-1">llama-factory(llama-board) 微调模型 <a class="header-anchor" href="#llama-factory-llama-board-微调模型" aria-label="Permalink to &quot;llama-factory(llama-board) 微调模型&quot;">​</a></h3><p>配置环境安装: 下面以qwen:7b-chat模型为例子, 用24G运存的GPU进行微调.</p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki material-theme-palenight"><code><span class="line"><span style="color:#FFCB6B;">git</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">clone</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">https://github.com/hiyouga/LLaMA-Factory.git</span></span>
<span class="line"><span style="color:#FFCB6B;">conda</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">create</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">-n</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">llama_factory</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">python=</span><span style="color:#F78C6C;">3.10</span></span>
<span class="line"><span style="color:#FFCB6B;">conda</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">activate</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">llama_factory</span></span>
<span class="line"><span style="color:#82AAFF;">cd</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">LLaMA-Factory</span></span>
<span class="line"><span style="color:#FFCB6B;">pip</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">install</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">-r</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">requirements.txt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 直接启动一个webui界面, 从0开始训练</span></span>
<span class="line"><span style="color:#A6ACCD;">CUDA_VISIBLE_DEVICES</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#A6ACCD;"> python src/train_web.py</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># webui界面可以把训练的命令参数保留下来, 用于下次的复现, 如下</span></span>
<span class="line"><span style="color:#A6ACCD;">CUDA_VISIBLE_DEVICES</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#A6ACCD;"> python src/train_bash.py \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#FFCB6B;">--stage</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">sft</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--do_train</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">True</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--model_name_or_path</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">qwen/Qwen1.</span><span style="color:#F78C6C;">5</span><span style="color:#C3E88D;">-7B-Chat</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--adapter_name_or_path</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">saves/Qwen1.</span><span style="color:#F78C6C;">5</span><span style="color:#C3E88D;">-7B-Chat/lora/train_2024-</span><span style="color:#F78C6C;">02</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">26</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">20</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">08</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">03</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--finetuning_type</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">lora</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--template</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">qwen</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--dataset_dir</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">data</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--dataset</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">self_cognition</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--cutoff_len</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--learning_rate</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0.0001</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--num_train_epochs</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">10.0</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--max_samples</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">100000</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--per_device_train_batch_size</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">4</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--gradient_accumulation_steps</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">4</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--lr_scheduler_type</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">cosine</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--max_grad_norm</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1.0</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--logging_steps</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">5</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--save_steps</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">100</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--warmup_steps</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--output_dir</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">saves/Qwen1.</span><span style="color:#F78C6C;">5</span><span style="color:#C3E88D;">-7B-Chat/lora/train_2024-</span><span style="color:#F78C6C;">02</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">26</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">20</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">08</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">03</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--fp16</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">True</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--lora_rank</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">8</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--lora_dropout</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0.1</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--lora_target</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">q_proj,v_proj</span><span style="color:#A6ACCD;"> \</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--plot_loss</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">True</span></span></code></pre></div><h3 id="webui-界面" tabindex="-1">webui 界面 <a class="header-anchor" href="#webui-界面" aria-label="Permalink to &quot;webui 界面&quot;">​</a></h3><p><img src="/assets/latest-llm-model-fine-tune-webui.df62f953.png" alt=""></p><h3 id="_3epoch训练身份效果" tabindex="-1">3epoch训练身份效果 <a class="header-anchor" href="#_3epoch训练身份效果" aria-label="Permalink to &quot;3epoch训练身份效果&quot;">​</a></h3><p>训练完后加载lora进行检查: <img src="/assets/3epoch.e5285bc7.png" alt="3epoch"></p><p><img src="/assets/latest-llm-model-fine-tune-3epoch.9f1afb33.png" alt=""></p><h3 id="_6epoch训练后效果" tabindex="-1">6epoch训练后效果 <a class="header-anchor" href="#_6epoch训练后效果" aria-label="Permalink to &quot;6epoch训练后效果&quot;">​</a></h3><p><img src="/assets/latest-llm-model-fine-tune-6epoch.08259eb9.png" alt=""></p><p><img src="/assets/latest-llm-model-fine-tune-6epoch-chat.c4ab5438.png" alt=""> 可见, 在80条身份数据的情况下6个epoch, 学习率1e-4 , lora rank为4的情况下, 其余默认, 已经让模型能够重新定义模型的“自我身份”. 说明微调起作用了. 之后也简单问了一下常规的问题, 以检查是否造成了灾难性遗忘: <img src="/assets/latest-llm-model-fine-tune-6epoch-try.8ca58604.png" alt=""> 发现还可以, 之前用ptuning 对chatglm进行微调的时候 ,就出现了很明显的灾难性遗忘, 原来的功能都丧失了. 虽然ptuninig v2 也只对prefix encoder做微调.(也可能还是因为数据量不够, epoch过多)</p><h3 id="_10epoch训练效果" tabindex="-1">10epoch训练效果 <a class="header-anchor" href="#_10epoch训练效果" aria-label="Permalink to &quot;10epoch训练效果&quot;">​</a></h3><p>10个epoch后, 就开始工作不正常了, 出现了. 数据量不够, 轮数太多, 过拟合了. <img src="/assets/latest-llm-model-fine-tune-10epoch.104e551b.png" alt=""></p><p><img src="/assets/iShot_2024-02-26_20.17.33.e0b8bc45.png" alt=""></p><h2 id="模型微调总结" tabindex="-1">模型微调总结 <a class="header-anchor" href="#模型微调总结" aria-label="Permalink to &quot;模型微调总结&quot;">​</a></h2><ul><li>硬件要求高: 全参微调7b大概需要2个V100; lora训练7b,需要至少22G显存; qlora训练至少需要16G;</li><li>数据量要求高: 虽然可以使用过chatgpt进行问答的生成, 但是如果是<strong>垂直领域</strong>, 那回答也依然需要人工精确的去核对. 而垂直领域恰恰是希望能够非常准确, 才能突出此<strong>模型应用</strong>有别于其他的价值.</li></ul><p>所以目前微调技术会从SFT--&gt; PEFT发展(lora -&gt; qlora). 往着降低成本, 减少微调时间, 加速实验,加速产出的方向走. 期待低成本微调有更大的技术突破, 或者显卡硬件价格下降. 😃</p><p>最后, 虽然但是, 无论是SFT还是PEFT, 模型微调依然是给模型赋予新功能绕不开的问题. 比如为6b版本的模型也赋予工具调用能力.下面的 llama factory的agent微调例子就很实用.</p><blockquote><p><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjG6PXh4MaEAxUhlK8BHSTHDkUQFnoECAYQAQ&amp;url=https%3A%2F%2Farxiv.org%2Fabs%2F2305.14314&amp;usg=AOvVaw0DPZGS_zRJAyr-clb7RXRc&amp;opi=89978449" target="_blank" rel="noreferrer">QLoRA: Efficient Finetuning of Quantized LLMs</a><a href="https://zhuanlan.zhihu.com/p/678989191?utm_medium=social&amp;utm_oi=63143405420544&amp;utm_psn=1735115630280503296&amp;utm_source=ZHShareTargetIDMore" target="_blank" rel="noreferrer">单卡 3 小时训练专属大模型 Agent：基于 LLaMA Factory 实战 - 知乎 (zhihu.com)</a></p></blockquote></div></div></main><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter" data-v-2bf54a99 data-v-3d1b5f7c><div class="container" data-v-3d1b5f7c><p class="message" data-v-3d1b5f7c>2024 - future</p><p class="copyright" data-v-3d1b5f7c>我在练习生活</p></div></footer><!--[--><!--]--></div></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"blog_archives.md\":\"623bcfb1\",\"blog_index.md\":\"6f622db4\",\"blog_authors_yi-ming.md\":\"72970469\",\"blog_posts_2024_vitepress_obsidian.md\":\"b7e2110e\",\"blog_tags.md\":\"62e1cd9b\",\"index.md\":\"3cf96156\",\"blog_posts_2024_llm_agent.md\":\"c277315c\",\"unpost_blog_gpt-sovits.md\":\"603b8d35\",\"unpost_blog_rag_enhance.md\":\"f9ba498a\",\"blog_posts_2024_latest-llm-model-fine-tune.md\":\"f2e579b3\",\"blog_posts_2024_ollama_local_inference_summary_generation_capability.md\":\"5e3bf348\",\"blog_posts_2024_isaac_image_recognition.md\":\"9fd3c83f\",\"blog_posts_2024_vitepress_use_vue_animation.md\":\"b70daf99\"}")
__VP_SITE_DATA__ = JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"YiMing's blog\",\"description\":\"A balanced life, coding for enjoyment\",\"base\":\"/\",\"head\":[],\"appearance\":true,\"themeConfig\":{\"logo\":\"yiminglogo2.png\",\"siteTitle\":\"YiMing's Blog\",\"footer\":{\"message\":\"2024 - future\",\"copyright\":\"我在练习生活\"},\"blog\":{\"title\":\"博文\",\"description\":\"✍️\",\"defaultAuthor\":\"YiMing\",\"categoryIcons\":{\"article\":\"i-[heroicons-outline/book-open]\",\"tutorial\":\"i-[heroicons-outline/academic-cap]\",\"document\":\"i-[heroicons-outline/annotation]\"},\"tagIcons\":{\"github\":\"i-[carbon/logo-github]\",\"vue\":\"i-[carbon/logo-vue]\"}},\"search\":{\"provider\":\"local\"},\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Blog\",\"items\":[{\"text\":\"Blog Home\",\"link\":\"/blog/\",\"activeMatch\":\"/blog/$\"},{\"text\":\"Tags\",\"link\":\"/blog/tags\",\"activeMatch\":\"/blog/tags\"},{\"text\":\"Archives\",\"link\":\"/blog/archives\",\"activeMatch\":\"/blog/archives\"}]},{\"text\":\"About\",\"link\":\"/blog/authors/yi-ming\"}],\"sidebar\":[{\"text\":\"Examples\",\"items\":[{\"text\":\"Markdown Examples\",\"link\":\"/markdown-examples\"},{\"text\":\"Runtime API Examples\",\"link\":\"/api-examples\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/realzhengyiming\"}]},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}")</script>
    
  </body>
</html>