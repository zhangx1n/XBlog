import{_ as s,c as a,o as l,V as n}from"./chunks/framework.41330901.js";const o="/assets/lora微调注入结构.8a777aea.png",p="/assets/对比lora-qlora.50c36916.png",e="/assets/latest-llm-model-fine-tune-webui.df62f953.png",t="/assets/3epoch.e5285bc7.png",r="/assets/latest-llm-model-fine-tune-3epoch.9f1afb33.png",c="/assets/latest-llm-model-fine-tune-6epoch.08259eb9.png",C="/assets/latest-llm-model-fine-tune-6epoch-chat.c4ab5438.png",i="/assets/latest-llm-model-fine-tune-6epoch-try.8ca58604.png",A="/assets/latest-llm-model-fine-tune-10epoch.104e551b.png",y="/assets/iShot_2024-02-26_20.17.33.e0b8bc45.png",q=JSON.parse('{"title":"主流llm大模型微调小结","description":"llm大模型微调总结","frontmatter":{"title":"主流llm大模型微调小结","datetime":"2024-02-25T00:00:00.000Z","time":1020,"description":"llm大模型微调总结","navbar":true,"sidebar":false,"footer":true,"date":"2024-02-25T00:00:00.000Z","author":"Yi Ming","category":"Document","next":false,"tags":["深度学习"],"blog":"post","aside":"left","prev":false},"headers":[],"relativePath":"blog/posts/2024/latest-llm-model-fine-tune.md"}'),h={name:"blog/posts/2024/latest-llm-model-fine-tune.md"},D=n('<h2 id="引言" tabindex="-1">引言 <a class="header-anchor" href="#引言" aria-label="Permalink to &quot;引言&quot;">​</a></h2><p>llm大模型目前比较火的两个垂直领域应用的技术路线: PEFT(参数有效微调)和RAG(检索增强生成). 目前这两个方向按实用性来说, 如果单看垂直领域知识库, 那确实RAG能以更低的成本和更快的速度应用起来.<br> 但是<strong>微调训练</strong>能为大语言模型增加新的功能, 比如翻译, 使用agent(agent工具调用微调), 所以它依然非常重要, 并且微调技术的增强和RAG是可以共同促进的. 下文就以简单的身份微调作为例子, 简单实践下llm微调.</p><hr><h2 id="大模型微调技术路线" tabindex="-1">大模型微调技术路线 <a class="header-anchor" href="#大模型微调技术路线" aria-label="Permalink to &quot;大模型微调技术路线&quot;">​</a></h2><h3 id="sft-有监督微调" tabindex="-1">SFT (有监督微调) <a class="header-anchor" href="#sft-有监督微调" aria-label="Permalink to &quot;SFT (有监督微调)&quot;">​</a></h3><p>简单的说就是<strong>传统全参数微调</strong>, 特点就是, 标记数据中将输入数据映射到期望的输出去. 计算成本低于预训练(从训练数据的量来说, 但是对于大模型来说,这个量依然很大) 例子:</p><ul><li>ImageNet数据集预训练后, 通过较少的数据集迁移到其他主题的数据集上的图像识别模型</li><li>stable diffusion 的dreambooth微调(全参)</li><li>llm fine-tune (比如bert 一般就是基于这种方式全参微调)</li></ul><h3 id="peft-参数有效微调" tabindex="-1">PEFT (参数有效微调) <a class="header-anchor" href="#peft-参数有效微调" aria-label="Permalink to &quot;PEFT (参数有效微调)&quot;">​</a></h3><p>旨在减少在微调过程中需要更新的参数数量, 通常是在微调的时候固定模型的一部分参数, 只更新一部分参数, 或者在原模型中增加一个可调的小型网络来对模型进行微调. 例子:</p><ul><li>Prefix Tuning</li></ul><ul><li>ptuning</li><li>ptuning v2</li><li>LoRA</li><li>stable diffusion 中的 text-inversion 、 hyperNetwork 、lora等技术</li><li>...</li></ul><h3 id="rlhf-人类反馈的强化学习" tabindex="-1">RLHF (人类反馈的强化学习) <a class="header-anchor" href="#rlhf-人类反馈的强化学习" aria-label="Permalink to &quot;RLHF (人类反馈的强化学习)&quot;">​</a></h3><p>chatgpt中用到的一种基于强化学习方法的技术. 激励模型训练，使语言模型补全与人工标签员的偏好保持一致。类似于, 水平高的人类围棋手就能够训练出水平高的围棋选手一样. rlhf也是, 通过人类对生成结果的偏好选择进行反馈, 然后让llm能够生成更多人类偏好的内容.  例子:</p><ul><li>chatgpt 重要的微调流程</li></ul><h2 id="目前比较主流的peft微调" tabindex="-1">目前比较主流的PEFT微调 <a class="header-anchor" href="#目前比较主流的peft微调" aria-label="Permalink to &quot;目前比较主流的PEFT微调&quot;">​</a></h2><p>目前的PEFT微调方法也还在不停的更新中, 不过截止到此文编写只是, lora应该是比较主流的. 对于llm大模型, 还有ptuning v2</p><h3 id="lora微调" tabindex="-1">LoRA微调 <a class="header-anchor" href="#lora微调" aria-label="Permalink to &quot;LoRA微调&quot;">​</a></h3><p>LoRA通过添加一个低秩线性适应层到原有模型，冻结原模型权重, 只对新增加的低秩矩阵进行微调，大大降低了微调的复杂度。lora微调的结构图如下, 它有效的原因是模型往往是过参数化的, 大部分的任务只需要改动少量权重(和对应新任务相关的权重) 就应该可以有效果. LoRA 不会改变层的所有组件中的权重矩阵 W，而是创建两个较小的矩阵 A 和 B，其乘积大致表示对 W 的修改。适应可以在数学上表示为 Y = W+AB. 通过微调AB就可以让模型适配新的任务.</p><p><img src="'+o+'" alt=""></p><h3 id="qlora" tabindex="-1">QLoRA <a class="header-anchor" href="#qlora" aria-label="Permalink to &quot;QLoRA&quot;">​</a></h3><p>QLoRA是 LoRA 的扩展版本，它的工作原理是将预训练LLM的权重参数的精度量化为 4 位精度。通常，训练模型的参数以 32 位格式存储，但 QLoRA 将它们压缩为 4 位格式。这减少了 LLM的内存占用，从而可以在单个 GPU 上对其进行微调。这种方法显著减少了内存占用，使得在功能较弱的硬件（包括消费类 GPU）上运行LLM模型成为可能。 <img src="'+p+`" alt=""></p><h3 id="lora-与qlora的对比" tabindex="-1">LoRA 与QLoRA的对比 <a class="header-anchor" href="#lora-与qlora的对比" aria-label="Permalink to &quot;LoRA 与QLoRA的对比&quot;">​</a></h3><p>LoRA优点:</p><ul><li>允许高效的模型微调，只需要更新模型中的小部分参数。</li><li>提供了参数扩展的灵活性，而不需要大规模改动原有的权重。<br> LoRA缺点:</li><li>相对于量化版本，存储和计算资源需求更高。</li><li>对于资源有限的环境来说，可能不如量化方法高效。<br> QLoRA优点:</li><li>通过量化减少了模型的内存占用和计算资源使用。</li><li>提高了模型在资源受限设备上的适用性和能效。<br> QLoRA缺点:</li><li>量化可能会引起模型精度的损失，尤其是在低比特宽度量化时。</li><li>需要仔细调优量化级别，以及可能在某些硬件上不提供性能优势。</li><li>并且QLoRA由于采用了低精度, 训练难度也会大一些</li></ul><h3 id="ptuning-v2" tabindex="-1">ptuning v2 <a class="header-anchor" href="#ptuning-v2" aria-label="Permalink to &quot;ptuning v2&quot;">​</a></h3><p>主流还是LoRA系列的. ptunning 目前好像主要chatglm在用. ptunning v2 相比于ptuning v1增加了注入的层数, 让ptuning在参数量10b以下的模型也能达到还不错的效果.</p><h2 id="llm-lora-训练" tabindex="-1">llm LoRA 训练 <a class="header-anchor" href="#llm-lora-训练" aria-label="Permalink to &quot;llm LoRA 训练&quot;">​</a></h2><ul><li>可以参考不同的模型的各自训练微调的说明</li><li>huggingface PEFT库</li><li>带Webui的 <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noreferrer">llama-factory</a> 体验也还可以</li><li>etc...</li></ul><h3 id="llama-factory-llama-board-微调模型" tabindex="-1">llama-factory(llama-board) 微调模型 <a class="header-anchor" href="#llama-factory-llama-board-微调模型" aria-label="Permalink to &quot;llama-factory(llama-board) 微调模型&quot;">​</a></h3><p>配置环境安装: 下面以qwen:7b-chat模型为例子, 用24G运存的GPU进行微调.</p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki material-theme-palenight"><code><span class="line"><span style="color:#FFCB6B;">git</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">clone</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">https://github.com/hiyouga/LLaMA-Factory.git</span></span>
<span class="line"><span style="color:#FFCB6B;">conda</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">create</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">-n</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">llama_factory</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">python=</span><span style="color:#F78C6C;">3.10</span></span>
<span class="line"><span style="color:#FFCB6B;">conda</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">activate</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">llama_factory</span></span>
<span class="line"><span style="color:#82AAFF;">cd</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">LLaMA-Factory</span></span>
<span class="line"><span style="color:#FFCB6B;">pip</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">install</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">-r</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">requirements.txt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># 直接启动一个webui界面, 从0开始训练</span></span>
<span class="line"><span style="color:#A6ACCD;">CUDA_VISIBLE_DEVICES</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#A6ACCD;"> python src/train_web.py</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># webui界面可以把训练的命令参数保留下来, 用于下次的复现, 如下</span></span>
<span class="line"><span style="color:#A6ACCD;">CUDA_VISIBLE_DEVICES</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#A6ACCD;"> python src/train_bash.py \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#FFCB6B;">--stage</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">sft</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--do_train</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">True</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--model_name_or_path</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">qwen/Qwen1.</span><span style="color:#F78C6C;">5</span><span style="color:#C3E88D;">-7B-Chat</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--adapter_name_or_path</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">saves/Qwen1.</span><span style="color:#F78C6C;">5</span><span style="color:#C3E88D;">-7B-Chat/lora/train_2024-</span><span style="color:#F78C6C;">02</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">26</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">20</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">08</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">03</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--finetuning_type</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">lora</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--template</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">qwen</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--dataset_dir</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">data</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--dataset</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">self_cognition</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--cutoff_len</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1024</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--learning_rate</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0.0001</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--num_train_epochs</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">10.0</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--max_samples</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">100000</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--per_device_train_batch_size</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">4</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--gradient_accumulation_steps</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">4</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--lr_scheduler_type</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">cosine</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--max_grad_norm</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1.0</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--logging_steps</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">5</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--save_steps</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">100</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--warmup_steps</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--output_dir</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">saves/Qwen1.</span><span style="color:#F78C6C;">5</span><span style="color:#C3E88D;">-7B-Chat/lora/train_2024-</span><span style="color:#F78C6C;">02</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">26</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">20</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">08</span><span style="color:#C3E88D;">-</span><span style="color:#F78C6C;">03</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--fp16</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">True</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--lora_rank</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">8</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--lora_dropout</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0.1</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--lora_target</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">q_proj,v_proj</span><span style="color:#A6ACCD;"> \\</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C3E88D;">--plot_loss</span><span style="color:#A6ACCD;"> </span><span style="color:#C3E88D;">True</span></span></code></pre></div><h3 id="webui-界面" tabindex="-1">webui 界面 <a class="header-anchor" href="#webui-界面" aria-label="Permalink to &quot;webui 界面&quot;">​</a></h3><p><img src="`+e+'" alt=""></p><h3 id="_3epoch训练身份效果" tabindex="-1">3epoch训练身份效果 <a class="header-anchor" href="#_3epoch训练身份效果" aria-label="Permalink to &quot;3epoch训练身份效果&quot;">​</a></h3><p>训练完后加载lora进行检查: <img src="'+t+'" alt="3epoch"></p><p><img src="'+r+'" alt=""></p><h3 id="_6epoch训练后效果" tabindex="-1">6epoch训练后效果 <a class="header-anchor" href="#_6epoch训练后效果" aria-label="Permalink to &quot;6epoch训练后效果&quot;">​</a></h3><p><img src="'+c+'" alt=""></p><p><img src="'+C+'" alt=""> 可见, 在80条身份数据的情况下6个epoch, 学习率1e-4 , lora rank为4的情况下, 其余默认, 已经让模型能够重新定义模型的“自我身份”. 说明微调起作用了. 之后也简单问了一下常规的问题, 以检查是否造成了灾难性遗忘: <img src="'+i+'" alt=""> 发现还可以, 之前用ptuning 对chatglm进行微调的时候 ,就出现了很明显的灾难性遗忘, 原来的功能都丧失了. 虽然ptuninig v2 也只对prefix encoder做微调.(也可能还是因为数据量不够, epoch过多)</p><h3 id="_10epoch训练效果" tabindex="-1">10epoch训练效果 <a class="header-anchor" href="#_10epoch训练效果" aria-label="Permalink to &quot;10epoch训练效果&quot;">​</a></h3><p>10个epoch后, 就开始工作不正常了, 出现了. 数据量不够, 轮数太多, 过拟合了. <img src="'+A+'" alt=""></p><p><img src="'+y+'" alt=""></p><h2 id="模型微调总结" tabindex="-1">模型微调总结 <a class="header-anchor" href="#模型微调总结" aria-label="Permalink to &quot;模型微调总结&quot;">​</a></h2><ul><li>硬件要求高: 全参微调7b大概需要2个V100; lora训练7b,需要至少22G显存; qlora训练至少需要16G;</li><li>数据量要求高: 虽然可以使用过chatgpt进行问答的生成, 但是如果是<strong>垂直领域</strong>, 那回答也依然需要人工精确的去核对. 而垂直领域恰恰是希望能够非常准确, 才能突出此<strong>模型应用</strong>有别于其他的价值.</li></ul><p>所以目前微调技术会从SFT--&gt; PEFT发展(lora -&gt; qlora). 往着降低成本, 减少微调时间, 加速实验,加速产出的方向走. 期待低成本微调有更大的技术突破, 或者显卡硬件价格下降. 😃</p><p>最后, 虽然但是, 无论是SFT还是PEFT, 模型微调依然是给模型赋予新功能绕不开的问题. 比如为6b版本的模型也赋予工具调用能力.下面的 llama factory的agent微调例子就很实用.</p><blockquote><p><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjG6PXh4MaEAxUhlK8BHSTHDkUQFnoECAYQAQ&amp;url=https%3A%2F%2Farxiv.org%2Fabs%2F2305.14314&amp;usg=AOvVaw0DPZGS_zRJAyr-clb7RXRc&amp;opi=89978449" target="_blank" rel="noreferrer">QLoRA: Efficient Finetuning of Quantized LLMs</a><a href="https://zhuanlan.zhihu.com/p/678989191?utm_medium=social&amp;utm_oi=63143405420544&amp;utm_psn=1735115630280503296&amp;utm_source=ZHShareTargetIDMore" target="_blank" rel="noreferrer">单卡 3 小时训练专属大模型 Agent：基于 LLaMA Factory 实战 - 知乎 (zhihu.com)</a></p></blockquote>',47),m=[D];function u(d,_,g,b,f,E){return l(),a("div",null,m)}const L=s(h,[["render",u]]);export{q as __pageData,L as default};
