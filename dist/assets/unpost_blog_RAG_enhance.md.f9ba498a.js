import{_ as e,c as t,o as a,V as n}from"./chunks/framework.41330901.js";const h=JSON.parse('{"title":"RAG增强优化思路","description":"rag enhance, 增强优化思路","frontmatter":{"title":"RAG增强优化思路","datetime":"2024-02-27T00:00:00.000Z","time":"09:10","description":"rag enhance, 增强优化思路","navbar":true,"sidebar":true,"footer":true,"date":"2024-02-27T00:00:00.000Z","author":"Yi Ming","category":"Document","next":true,"tags":["深度学习"]},"headers":[],"relativePath":"unpost_blog/RAG_enhance.md"}'),l={name:"unpost_blog/RAG_enhance.md"},r=n("<p>此处直接写前言, 所谓的前言就是写这篇文章的目的? 什么情况想到了做这件事情, 然后才开始标题和引言. 也可以作为一个小分段了</p><hr><p>RAG 也就能从</p><p>召回方向: 提高召回准确率 2. 依然使用文本向量数据库的情况下: 3. 对向量数据库使用带语言意义的主题分割, 形成主题树,类似llamaindex, 多级树索引, 索引的元数据上放着对应内容的概括内容进行embedding. 优化大文档的速度,和准确性(这种方法可以让更小粒度的文段进行embedding); 问句也需要让llm进行拆解成子问题, 每个子问题进行索引. 4. 改用结构型数据库, 比如增加agent, 使用关系型据库, 或者传统的图数据库. 一样,需要 nltk库做 语义的拆解, 实体的识别, 然后转化为图数据库的搜索语句, 关键词 + 复合搜索, 把几跳的结构都搜索出来. (关系搜索出来) ; 或者更准确的意图搜索出来. 再传给llm 5. 对自己专有领域的文本, 做自行的embedding微调, 以识别有别于广度知识的文本结构.虽然现在开源的bge-large 应该已经可以完成大部分的问题 6. 相关性的召回,过滤 模型回答方向:</p><ul><li>使用支持更长上下文的llm(离线的了, 那就都用吧), 目的是能够放下更多的参考资料,</li><li>微调特定的格式指令,以避免模型被相关内容中的指令污染, (一般大模型就没这个问题)</li><li>使用更大量级的模型, 比如13b, 7b有问题, chatgpt就没问题. 使</li><li>意图识别, 增加额外判断,用户所需要的答案是否在相关文章中, 如果不在, 根据已有的文章知识, 回问用户,以让用户提供足够的定语, 反问用户. 对用户模凌两可的(需要对多轮对话进行微调, 成本确实就更高了.)</li><li>继续持续关注最新的技术和知识</li></ul><p>我自己的话, qwen, chatglm , 微调过. stable lora 的 alpha 一般是rank的一倍, 避免过拟合. 32 就可以了 lora微调是对lora权重层的微调.</p>",6),o=[r];function i(_,c,s,p,d,m){return a(),t("div",null,o)}const u=e(l,[["render",i]]);export{h as __pageData,u as default};
